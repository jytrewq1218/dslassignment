{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0]])\n",
    "y_true = np.array([[0,1,0,0],[0,0,1,0],[0,0,0,1]])\n",
    "vector_dim = 4\n",
    "hidden_dim = 3\n",
    "time_step = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2],\n",
       "       [2, 2, 2, 2],\n",
       "       [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 1, 1, 1])\n",
    "b = np.array([[2], [2], [2]])\n",
    "a*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, vector_dim, hidden_dim, time_step):\n",
    "        self.vector_dim = vector_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.time_step = time_step\n",
    "        self.W_H = np.random.uniform(0, 1,(hidden_dim, hidden_dim))\n",
    "        self.W_X = np.random.uniform(0, 1,(hidden_dim, vector_dim))\n",
    "        self.W_Y = np.random.uniform(0, 1,(vector_dim, hidden_dim))\n",
    "        self.b_h = np.random.uniform(0, 1,(hidden_dim,))\n",
    "        self.b_y = np.random.uniform(0, 1,(vector_dim,))\n",
    "        \n",
    "    def forward_propagation(self, inputs):\n",
    "        h = np.zeros((self.time_step + 1, self.hidden_dim))\n",
    "        y = np.zeros((self.time_step, self.vector_dim))\n",
    "        for time in np.arange(1, self.time_step + 1):\n",
    "            h[time] = np.tanh(self.W_H @ h[time - 1] + self.W_X @ inputs[time - 1] + self.b_h)\n",
    "            y[time - 1] = np.exp(self.W_Y @ h[time] + self.b_y) / np.sum(np.exp(self.W_Y @ h[time] + self.b_y))\n",
    "        return h, y\n",
    "\n",
    "    def cross_entropy(self, inputs, y_true):\n",
    "        p = np.zeros(self.hidden_dim)\n",
    "        h, y = self.forward_propagation(inputs)\n",
    "        for time in np.arange(self.time_step):\n",
    "            p[time] = y[time] @ np.transpose(np.log(np.exp(y[time]) / np.sum(np.exp(y[time]))))\n",
    "        L = -np.sum(p)\n",
    "        return L\n",
    "    \n",
    "    def back_propagation(self, inputs, y_true, learn_rate):\n",
    "        h, y = self.forward_propagation(inputs)\n",
    "        dLdW_H = np.zeros(self.W_H.shape)\n",
    "        dLdW_X = np.zeros(self.W_X.shape)\n",
    "        dLdW_Y = np.zeros(self.W_Y.shape)\n",
    "        dLdb_h = np.zeros(self.b_h.shape)\n",
    "        dLdb_y = np.zeros(self.b_y.shape)\n",
    "        for time in np.arange(self.time_step):\n",
    "            dLdW_H += (y[time] - y_true[time]) @ self.W_Y * (1 - h[time + 1] * h[time + 1]) * h[time]\n",
    "            dLdW_X += np.reshape((y[time] - y_true[time]) @ self.W_Y * (1 - h[time + 1] * h[time + 1]), (3, 1)) * inputs[time]\n",
    "            dLdW_Y += np.reshape(y[time] - y_true[time], (4, 1)) * h[time + 1]\n",
    "            dLdb_h += np.transpose((y[time] - y_true[time]) @ self.W_Y * (1 - h[time + 1] * h[time + 1]))\n",
    "            dLdb_y += np.transpose(y[time] - y_true[time])\n",
    "        self.W_H -= learn_rate * dLdW_H\n",
    "        self.W_X -= learn_rate * dLdW_X\n",
    "        self.W_Y -= learn_rate * dLdW_Y\n",
    "        self.b_h -= learn_rate * dLdb_h\n",
    "        self.b_y -= learn_rate * dLdb_y\n",
    "        \n",
    "def train(model, inputs, y_true, learn_rate = 0.1, epoches = 100):\n",
    "    losses = []\n",
    "    for epoch in range(epoches):\n",
    "        print(f'Begin learning epoch {epoch}')\n",
    "        model.back_propagation(inputs, y_true, learn_rate)\n",
    "        loss = model.cross_entropy(inputs, y_true)\n",
    "        losses.append(loss)\n",
    "        time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f'{time} : Loss at epoch {epoch} = {loss}')\n",
    "        print(f'End learning epoch {epoch}')\n",
    "        print('---------------------------')\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(vector_dim, hidden_dim, time_step)\n",
    "h, y = model.forward_propagation(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "[[0.2721187  0.32877908 0.25137964 0.14772259]\n",
      " [0.2370543  0.39433833 0.24595425 0.12265312]\n",
      " [0.23221637 0.40435557 0.2453601  0.11806795]]\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = model.cross_entropy(input_vector, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.07497984306974\n"
     ]
    }
   ],
   "source": [
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weight and bias\n",
      "----------W_H----------\n",
      "[[0.0506515  0.92189155 0.22342274]\n",
      " [0.42695093 0.32378633 0.96602726]\n",
      " [0.03590519 0.66895484 0.73683302]]\n",
      "----------W_X----------\n",
      "[[0.06795594 0.3264155  0.78022492 0.97990412]\n",
      " [0.01320748 0.10712714 0.94801455 0.14705456]\n",
      " [0.24830995 0.02878848 0.90719933 0.19667378]]\n",
      "----------W_Y----------\n",
      "[[0.28754184 0.27060449 0.85209841]\n",
      " [0.16592032 0.99731122 0.90585116]\n",
      " [0.30949463 0.49231608 0.81875666]\n",
      " [0.12620462 0.32187103 0.8518318 ]]\n",
      "----------b_h----------\n",
      "[0.33801231 0.42273355 0.72840862]\n",
      "----------b_y----------\n",
      "[0.58329317 0.48071854 0.4296648  0.01367993]\n"
     ]
    }
   ],
   "source": [
    "print('Initial weight and bias')\n",
    "print('----------W_H----------')\n",
    "print(model.W_H)\n",
    "print('----------W_X----------')\n",
    "print(model.W_X)\n",
    "print('----------W_Y----------')\n",
    "print(model.W_Y)\n",
    "print('----------b_h----------')\n",
    "print(model.b_h)\n",
    "print('----------b_y----------')\n",
    "print(model.b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.back_propagation(input_vector, y_true, learn_rate = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weight and bias(epoch 1)\n",
      "----------W_H----------\n",
      "[[0.05616255 0.91020821 0.21804029]\n",
      " [0.43246198 0.31210299 0.96064481]\n",
      " [0.04141624 0.65727149 0.73145057]]\n",
      "----------W_X----------\n",
      "[[0.01402349 0.35086429 0.77559309 0.97990412]\n",
      " [0.36623168 0.08651223 0.94448668 0.14705456]\n",
      " [0.26766933 0.02180944 0.90705099 0.19667378]]\n",
      "----------W_Y----------\n",
      "[[-0.2436699  -0.28864124  0.19813768]\n",
      " [-0.30263567  0.51015251  0.64478575]\n",
      " [ 0.60854459  0.83468722  1.08067445]\n",
      " [ 0.82692239  1.02590432  1.50494014]]\n",
      "----------b_h----------\n",
      "[0.30389682 0.75161497 0.74064062]\n",
      "----------b_y----------\n",
      "[-0.15809621  0.35324556  0.68697081  0.62523626]\n"
     ]
    }
   ],
   "source": [
    "print('Learned weight and bias(epoch 1)')\n",
    "print('----------W_H----------')\n",
    "print(model.W_H)\n",
    "print('----------W_X----------')\n",
    "print(model.W_X)\n",
    "print('----------W_Y----------')\n",
    "print(model.W_Y)\n",
    "print('----------b_h----------')\n",
    "print(model.b_h)\n",
    "print('----------b_y----------')\n",
    "print(model.b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin learning epoch 0\n",
      "2021-09-03 10:27:08 : Loss at epoch 0 = 4.138641773779216\n",
      "End learning epoch 0\n",
      "---------------------------\n",
      "Begin learning epoch 1\n",
      "2021-09-03 10:27:08 : Loss at epoch 1 = 4.129302433168679\n",
      "End learning epoch 1\n",
      "---------------------------\n",
      "Begin learning epoch 2\n",
      "2021-09-03 10:27:08 : Loss at epoch 2 = 4.112541496168426\n",
      "End learning epoch 2\n",
      "---------------------------\n",
      "Begin learning epoch 3\n",
      "2021-09-03 10:27:08 : Loss at epoch 3 = 4.09542702200511\n",
      "End learning epoch 3\n",
      "---------------------------\n",
      "Begin learning epoch 4\n",
      "2021-09-03 10:27:08 : Loss at epoch 4 = 4.079921521467235\n",
      "End learning epoch 4\n",
      "---------------------------\n",
      "Begin learning epoch 5\n",
      "2021-09-03 10:27:08 : Loss at epoch 5 = 4.066438001830737\n",
      "End learning epoch 5\n",
      "---------------------------\n",
      "Begin learning epoch 6\n",
      "2021-09-03 10:27:08 : Loss at epoch 6 = 4.054872148779503\n",
      "End learning epoch 6\n",
      "---------------------------\n",
      "Begin learning epoch 7\n",
      "2021-09-03 10:27:08 : Loss at epoch 7 = 4.044966162111704\n",
      "End learning epoch 7\n",
      "---------------------------\n",
      "Begin learning epoch 8\n",
      "2021-09-03 10:27:08 : Loss at epoch 8 = 4.036444977123216\n",
      "End learning epoch 8\n",
      "---------------------------\n",
      "Begin learning epoch 9\n",
      "2021-09-03 10:27:08 : Loss at epoch 9 = 4.029063039115975\n",
      "End learning epoch 9\n",
      "---------------------------\n",
      "Begin learning epoch 10\n",
      "2021-09-03 10:27:08 : Loss at epoch 10 = 4.022615161390099\n",
      "End learning epoch 10\n",
      "---------------------------\n",
      "Begin learning epoch 11\n",
      "2021-09-03 10:27:08 : Loss at epoch 11 = 4.016933937548086\n",
      "End learning epoch 11\n",
      "---------------------------\n",
      "Begin learning epoch 12\n",
      "2021-09-03 10:27:08 : Loss at epoch 12 = 4.0118830149721125\n",
      "End learning epoch 12\n",
      "---------------------------\n",
      "Begin learning epoch 13\n",
      "2021-09-03 10:27:08 : Loss at epoch 13 = 4.007349760497233\n",
      "End learning epoch 13\n",
      "---------------------------\n",
      "Begin learning epoch 14\n",
      "2021-09-03 10:27:08 : Loss at epoch 14 = 4.003238345457413\n",
      "End learning epoch 14\n",
      "---------------------------\n",
      "Begin learning epoch 15\n",
      "2021-09-03 10:27:08 : Loss at epoch 15 = 3.999463196041111\n",
      "End learning epoch 15\n",
      "---------------------------\n",
      "Begin learning epoch 16\n",
      "2021-09-03 10:27:08 : Loss at epoch 16 = 3.9959422343872504\n",
      "End learning epoch 16\n",
      "---------------------------\n",
      "Begin learning epoch 17\n",
      "2021-09-03 10:27:08 : Loss at epoch 17 = 3.9925889712009246\n",
      "End learning epoch 17\n",
      "---------------------------\n",
      "Begin learning epoch 18\n",
      "2021-09-03 10:27:08 : Loss at epoch 18 = 3.9893021204224666\n",
      "End learning epoch 18\n",
      "---------------------------\n",
      "Begin learning epoch 19\n",
      "2021-09-03 10:27:08 : Loss at epoch 19 = 3.9859509700885787\n",
      "End learning epoch 19\n",
      "---------------------------\n",
      "Begin learning epoch 20\n",
      "2021-09-03 10:27:08 : Loss at epoch 20 = 3.98235449487502\n",
      "End learning epoch 20\n",
      "---------------------------\n",
      "Begin learning epoch 21\n",
      "2021-09-03 10:27:08 : Loss at epoch 21 = 3.9782529083467058\n",
      "End learning epoch 21\n",
      "---------------------------\n",
      "Begin learning epoch 22\n",
      "2021-09-03 10:27:08 : Loss at epoch 22 = 3.973273745731267\n",
      "End learning epoch 22\n",
      "---------------------------\n",
      "Begin learning epoch 23\n",
      "2021-09-03 10:27:08 : Loss at epoch 23 = 3.9669031600640965\n",
      "End learning epoch 23\n",
      "---------------------------\n",
      "Begin learning epoch 24\n",
      "2021-09-03 10:27:08 : Loss at epoch 24 = 3.958487506479595\n",
      "End learning epoch 24\n",
      "---------------------------\n",
      "Begin learning epoch 25\n",
      "2021-09-03 10:27:08 : Loss at epoch 25 = 3.947301811677705\n",
      "End learning epoch 25\n",
      "---------------------------\n",
      "Begin learning epoch 26\n",
      "2021-09-03 10:27:08 : Loss at epoch 26 = 3.9327069667737837\n",
      "End learning epoch 26\n",
      "---------------------------\n",
      "Begin learning epoch 27\n",
      "2021-09-03 10:27:08 : Loss at epoch 27 = 3.914358061881299\n",
      "End learning epoch 27\n",
      "---------------------------\n",
      "Begin learning epoch 28\n",
      "2021-09-03 10:27:08 : Loss at epoch 28 = 3.892357646496775\n",
      "End learning epoch 28\n",
      "---------------------------\n",
      "Begin learning epoch 29\n",
      "2021-09-03 10:27:08 : Loss at epoch 29 = 3.8672547325118876\n",
      "End learning epoch 29\n",
      "---------------------------\n",
      "Begin learning epoch 30\n",
      "2021-09-03 10:27:08 : Loss at epoch 30 = 3.839891889359716\n",
      "End learning epoch 30\n",
      "---------------------------\n",
      "Begin learning epoch 31\n",
      "2021-09-03 10:27:08 : Loss at epoch 31 = 3.811197028298497\n",
      "End learning epoch 31\n",
      "---------------------------\n",
      "Begin learning epoch 32\n",
      "2021-09-03 10:27:08 : Loss at epoch 32 = 3.7820176608312868\n",
      "End learning epoch 32\n",
      "---------------------------\n",
      "Begin learning epoch 33\n",
      "2021-09-03 10:27:08 : Loss at epoch 33 = 3.7530352327763437\n",
      "End learning epoch 33\n",
      "---------------------------\n",
      "Begin learning epoch 34\n",
      "2021-09-03 10:27:08 : Loss at epoch 34 = 3.7247465461008282\n",
      "End learning epoch 34\n",
      "---------------------------\n",
      "Begin learning epoch 35\n",
      "2021-09-03 10:27:08 : Loss at epoch 35 = 3.69748316930671\n",
      "End learning epoch 35\n",
      "---------------------------\n",
      "Begin learning epoch 36\n",
      "2021-09-03 10:27:08 : Loss at epoch 36 = 3.6714450228067177\n",
      "End learning epoch 36\n",
      "---------------------------\n",
      "Begin learning epoch 37\n",
      "2021-09-03 10:27:08 : Loss at epoch 37 = 3.646734569858682\n",
      "End learning epoch 37\n",
      "---------------------------\n",
      "Begin learning epoch 38\n",
      "2021-09-03 10:27:08 : Loss at epoch 38 = 3.623385867458305\n",
      "End learning epoch 38\n",
      "---------------------------\n",
      "Begin learning epoch 39\n",
      "2021-09-03 10:27:08 : Loss at epoch 39 = 3.6013870895226114\n",
      "End learning epoch 39\n",
      "---------------------------\n",
      "Begin learning epoch 40\n",
      "2021-09-03 10:27:08 : Loss at epoch 40 = 3.5806970632295276\n",
      "End learning epoch 40\n",
      "---------------------------\n",
      "Begin learning epoch 41\n",
      "2021-09-03 10:27:08 : Loss at epoch 41 = 3.5612569766963387\n",
      "End learning epoch 41\n",
      "---------------------------\n",
      "Begin learning epoch 42\n",
      "2021-09-03 10:27:08 : Loss at epoch 42 = 3.5429984436688517\n",
      "End learning epoch 42\n",
      "---------------------------\n",
      "Begin learning epoch 43\n",
      "2021-09-03 10:27:08 : Loss at epoch 43 = 3.525848925889351\n",
      "End learning epoch 43\n",
      "---------------------------\n",
      "Begin learning epoch 44\n",
      "2021-09-03 10:27:08 : Loss at epoch 44 = 3.509735286993819\n",
      "End learning epoch 44\n",
      "---------------------------\n",
      "Begin learning epoch 45\n",
      "2021-09-03 10:27:08 : Loss at epoch 45 = 3.4945860486841624\n",
      "End learning epoch 45\n",
      "---------------------------\n",
      "Begin learning epoch 46\n",
      "2021-09-03 10:27:08 : Loss at epoch 46 = 3.48033275843217\n",
      "End learning epoch 46\n",
      "---------------------------\n",
      "Begin learning epoch 47\n",
      "2021-09-03 10:27:08 : Loss at epoch 47 = 3.466910756986704\n",
      "End learning epoch 47\n",
      "---------------------------\n",
      "Begin learning epoch 48\n",
      "2021-09-03 10:27:08 : Loss at epoch 48 = 3.454259546302159\n",
      "End learning epoch 48\n",
      "---------------------------\n",
      "Begin learning epoch 49\n",
      "2021-09-03 10:27:08 : Loss at epoch 49 = 3.442322896263865\n",
      "End learning epoch 49\n",
      "---------------------------\n",
      "Begin learning epoch 50\n",
      "2021-09-03 10:27:08 : Loss at epoch 50 = 3.4310487849373157\n",
      "End learning epoch 50\n",
      "---------------------------\n",
      "Begin learning epoch 51\n",
      "2021-09-03 10:27:08 : Loss at epoch 51 = 3.420389236704837\n",
      "End learning epoch 51\n",
      "---------------------------\n",
      "Begin learning epoch 52\n",
      "2021-09-03 10:27:08 : Loss at epoch 52 = 3.410300101648292\n",
      "End learning epoch 52\n",
      "---------------------------\n",
      "Begin learning epoch 53\n",
      "2021-09-03 10:27:08 : Loss at epoch 53 = 3.4007408050663392\n",
      "End learning epoch 53\n",
      "---------------------------\n",
      "Begin learning epoch 54\n",
      "2021-09-03 10:27:08 : Loss at epoch 54 = 3.3916740860840804\n",
      "End learning epoch 54\n",
      "---------------------------\n",
      "Begin learning epoch 55\n",
      "2021-09-03 10:27:08 : Loss at epoch 55 = 3.383065737525129\n",
      "End learning epoch 55\n",
      "---------------------------\n",
      "Begin learning epoch 56\n",
      "2021-09-03 10:27:08 : Loss at epoch 56 = 3.374884354599943\n",
      "End learning epoch 56\n",
      "---------------------------\n",
      "Begin learning epoch 57\n",
      "2021-09-03 10:27:08 : Loss at epoch 57 = 3.3671010968459667\n",
      "End learning epoch 57\n",
      "---------------------------\n",
      "Begin learning epoch 58\n",
      "2021-09-03 10:27:08 : Loss at epoch 58 = 3.35968946566871\n",
      "End learning epoch 58\n",
      "---------------------------\n",
      "Begin learning epoch 59\n",
      "2021-09-03 10:27:08 : Loss at epoch 59 = 3.3526250984555803\n",
      "End learning epoch 59\n",
      "---------------------------\n",
      "Begin learning epoch 60\n",
      "2021-09-03 10:27:08 : Loss at epoch 60 = 3.345885579343255\n",
      "End learning epoch 60\n",
      "---------------------------\n",
      "Begin learning epoch 61\n",
      "2021-09-03 10:27:08 : Loss at epoch 61 = 3.339450266160114\n",
      "End learning epoch 61\n",
      "---------------------------\n",
      "Begin learning epoch 62\n",
      "2021-09-03 10:27:08 : Loss at epoch 62 = 3.3333001327313507\n",
      "End learning epoch 62\n",
      "---------------------------\n",
      "Begin learning epoch 63\n",
      "2021-09-03 10:27:08 : Loss at epoch 63 = 3.3274176255518206\n",
      "End learning epoch 63\n",
      "---------------------------\n",
      "Begin learning epoch 64\n",
      "2021-09-03 10:27:08 : Loss at epoch 64 = 3.3217865337499024\n",
      "End learning epoch 64\n",
      "---------------------------\n",
      "Begin learning epoch 65\n",
      "2021-09-03 10:27:08 : Loss at epoch 65 = 3.316391871249373\n",
      "End learning epoch 65\n",
      "---------------------------\n",
      "Begin learning epoch 66\n",
      "2021-09-03 10:27:08 : Loss at epoch 66 = 3.3112197700616806\n",
      "End learning epoch 66\n",
      "---------------------------\n",
      "Begin learning epoch 67\n",
      "2021-09-03 10:27:08 : Loss at epoch 67 = 3.3062573836915625\n",
      "End learning epoch 67\n",
      "---------------------------\n",
      "Begin learning epoch 68\n",
      "2021-09-03 10:27:08 : Loss at epoch 68 = 3.301492799703673\n",
      "End learning epoch 68\n",
      "---------------------------\n",
      "Begin learning epoch 69\n",
      "2021-09-03 10:27:08 : Loss at epoch 69 = 3.2969149605692953\n",
      "End learning epoch 69\n",
      "---------------------------\n",
      "Begin learning epoch 70\n",
      "2021-09-03 10:27:08 : Loss at epoch 70 = 3.2925135919855295\n",
      "End learning epoch 70\n",
      "---------------------------\n",
      "Begin learning epoch 71\n",
      "2021-09-03 10:27:08 : Loss at epoch 71 = 3.288279137931383\n",
      "End learning epoch 71\n",
      "---------------------------\n",
      "Begin learning epoch 72\n",
      "2021-09-03 10:27:08 : Loss at epoch 72 = 3.2842027017940723\n",
      "End learning epoch 72\n",
      "---------------------------\n",
      "Begin learning epoch 73\n",
      "2021-09-03 10:27:08 : Loss at epoch 73 = 3.2802759929635066\n",
      "End learning epoch 73\n",
      "---------------------------\n",
      "Begin learning epoch 74\n",
      "2021-09-03 10:27:08 : Loss at epoch 74 = 3.2764912783527116\n",
      "End learning epoch 74\n",
      "---------------------------\n",
      "Begin learning epoch 75\n",
      "2021-09-03 10:27:08 : Loss at epoch 75 = 3.272841338356857\n",
      "End learning epoch 75\n",
      "---------------------------\n",
      "Begin learning epoch 76\n",
      "2021-09-03 10:27:08 : Loss at epoch 76 = 3.2693194268134103\n",
      "End learning epoch 76\n",
      "---------------------------\n",
      "Begin learning epoch 77\n",
      "2021-09-03 10:27:08 : Loss at epoch 77 = 3.2659192345711294\n",
      "End learning epoch 77\n",
      "---------------------------\n",
      "Begin learning epoch 78\n",
      "2021-09-03 10:27:08 : Loss at epoch 78 = 3.2626348563162795\n",
      "End learning epoch 78\n",
      "---------------------------\n",
      "Begin learning epoch 79\n",
      "2021-09-03 10:27:08 : Loss at epoch 79 = 3.2594607603409687\n",
      "End learning epoch 79\n",
      "---------------------------\n",
      "Begin learning epoch 80\n",
      "2021-09-03 10:27:08 : Loss at epoch 80 = 3.2563917609711437\n",
      "End learning epoch 80\n",
      "---------------------------\n",
      "Begin learning epoch 81\n",
      "2021-09-03 10:27:08 : Loss at epoch 81 = 3.2534229934009185\n",
      "End learning epoch 81\n",
      "---------------------------\n",
      "Begin learning epoch 82\n",
      "2021-09-03 10:27:08 : Loss at epoch 82 = 3.2505498907057158\n",
      "End learning epoch 82\n",
      "---------------------------\n",
      "Begin learning epoch 83\n",
      "2021-09-03 10:27:08 : Loss at epoch 83 = 3.2477681628295\n",
      "End learning epoch 83\n",
      "---------------------------\n",
      "Begin learning epoch 84\n",
      "2021-09-03 10:27:08 : Loss at epoch 84 = 3.245073777361339\n",
      "End learning epoch 84\n",
      "---------------------------\n",
      "Begin learning epoch 85\n",
      "2021-09-03 10:27:08 : Loss at epoch 85 = 3.2424629419336877\n",
      "End learning epoch 85\n",
      "---------------------------\n",
      "Begin learning epoch 86\n",
      "2021-09-03 10:27:08 : Loss at epoch 86 = 3.239932088089268\n",
      "End learning epoch 86\n",
      "---------------------------\n",
      "Begin learning epoch 87\n",
      "2021-09-03 10:27:08 : Loss at epoch 87 = 3.2374778564750235\n",
      "End learning epoch 87\n",
      "---------------------------\n",
      "Begin learning epoch 88\n",
      "2021-09-03 10:27:08 : Loss at epoch 88 = 3.235097083230161\n",
      "End learning epoch 88\n",
      "---------------------------\n",
      "Begin learning epoch 89\n",
      "2021-09-03 10:27:08 : Loss at epoch 89 = 3.232786787440225\n",
      "End learning epoch 89\n",
      "---------------------------\n",
      "Begin learning epoch 90\n",
      "2021-09-03 10:27:08 : Loss at epoch 90 = 3.230544159529642\n",
      "End learning epoch 90\n",
      "---------------------------\n",
      "Begin learning epoch 91\n",
      "2021-09-03 10:27:08 : Loss at epoch 91 = 3.2283665504600343\n",
      "End learning epoch 91\n",
      "---------------------------\n",
      "Begin learning epoch 92\n",
      "2021-09-03 10:27:08 : Loss at epoch 92 = 3.226251461588727\n",
      "End learning epoch 92\n",
      "---------------------------\n",
      "Begin learning epoch 93\n",
      "2021-09-03 10:27:08 : Loss at epoch 93 = 3.2241965350183572\n",
      "End learning epoch 93\n",
      "---------------------------\n",
      "Begin learning epoch 94\n",
      "2021-09-03 10:27:08 : Loss at epoch 94 = 3.2221995442297926\n",
      "End learning epoch 94\n",
      "---------------------------\n",
      "Begin learning epoch 95\n",
      "2021-09-03 10:27:08 : Loss at epoch 95 = 3.22025838472977\n",
      "End learning epoch 95\n",
      "---------------------------\n",
      "Begin learning epoch 96\n",
      "2021-09-03 10:27:08 : Loss at epoch 96 = 3.218371064351368\n",
      "End learning epoch 96\n",
      "---------------------------\n",
      "Begin learning epoch 97\n",
      "2021-09-03 10:27:08 : Loss at epoch 97 = 3.2165356927036495\n",
      "End learning epoch 97\n",
      "---------------------------\n",
      "Begin learning epoch 98\n",
      "2021-09-03 10:27:08 : Loss at epoch 98 = 3.21475046905204\n",
      "End learning epoch 98\n",
      "---------------------------\n",
      "Begin learning epoch 99\n",
      "2021-09-03 10:27:08 : Loss at epoch 99 = 3.2130136675850496\n",
      "End learning epoch 99\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "model = RNN(vector_dim, hidden_dim, time_step)\n",
    "losses = train(model, input_vector, y_true)\n",
    "h, y = model.forward_propagation(input_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss versus Epoch plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Loss versus Epoch plot')\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weight and bias(epoch 10)\n",
      "----------W_H----------\n",
      "[[ 0.50932202 -0.00784417  0.69199169]\n",
      " [ 0.85165367  0.21513526  1.0278194 ]\n",
      " [ 0.82773446 -0.04777967  0.15304038]]\n",
      "----------W_X----------\n",
      "[[ 0.59642522  0.48991375  1.03489729  0.95379447]\n",
      " [-0.7827634   1.09530943  0.85136735  0.65522589]\n",
      " [ 0.44655709  0.39688764  0.20940656  0.39430302]]\n",
      "----------W_Y----------\n",
      "[[-0.03990112 -0.14628696 -0.79365113]\n",
      " [ 0.14218846 -1.76612166  0.78341169]\n",
      " [ 0.34264126  1.75889412  0.87861401]\n",
      " [ 1.04998565  1.87026819  0.46099954]]\n",
      "----------b_h----------\n",
      "[-0.10825366 -0.80765547  0.9862762 ]\n",
      "----------b_y----------\n",
      "[-0.36777499  1.13880324  0.63198311  0.33640707]\n"
     ]
    }
   ],
   "source": [
    "print('Learned weight and bias(epoch 10)')\n",
    "print('----------W_H----------')\n",
    "print(model.W_H)\n",
    "print('----------W_X----------')\n",
    "print(model.W_X)\n",
    "print('----------W_Y----------')\n",
    "print(model.W_Y)\n",
    "print('----------b_h----------')\n",
    "print(model.b_h)\n",
    "print('----------b_y----------')\n",
    "print(model.b_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction and True value of Y\n",
      "------Prediction-------\n",
      "[[0.01064884 0.94595399 0.02650433 0.01689284]\n",
      " [0.00481235 0.02747222 0.48395868 0.48375676]\n",
      " [0.00380065 0.01975769 0.46595152 0.51049014]]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]]\n",
      "---------True----------\n",
      "[[0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print('Prediction and True value of Y')\n",
    "print('------Prediction-------')\n",
    "print(y)\n",
    "print(np.round(y, 0))\n",
    "print('---------True----------')\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
